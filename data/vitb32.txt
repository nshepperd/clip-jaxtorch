CLIP(
  (visual): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
positional_embedding torch.Size([77, 512])
text_projection torch.Size([512, 512])
logit_scale torch.Size([])
visual.class_embedding torch.Size([768])
visual.positional_embedding torch.Size([50, 768])
visual.proj torch.Size([768, 512])
visual.conv1.weight torch.Size([768, 3, 32, 32])
visual.ln_pre.weight torch.Size([768])
visual.ln_pre.bias torch.Size([768])
visual.transformer.resblocks.0.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.0.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.0.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.0.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.0.ln_1.weight torch.Size([768])
visual.transformer.resblocks.0.ln_1.bias torch.Size([768])
visual.transformer.resblocks.0.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.0.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.0.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.0.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.0.ln_2.weight torch.Size([768])
visual.transformer.resblocks.0.ln_2.bias torch.Size([768])
visual.transformer.resblocks.1.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.1.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.1.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.1.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.1.ln_1.weight torch.Size([768])
visual.transformer.resblocks.1.ln_1.bias torch.Size([768])
visual.transformer.resblocks.1.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.1.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.1.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.1.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.1.ln_2.weight torch.Size([768])
visual.transformer.resblocks.1.ln_2.bias torch.Size([768])
visual.transformer.resblocks.2.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.2.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.2.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.2.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.2.ln_1.weight torch.Size([768])
visual.transformer.resblocks.2.ln_1.bias torch.Size([768])
visual.transformer.resblocks.2.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.2.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.2.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.2.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.2.ln_2.weight torch.Size([768])
visual.transformer.resblocks.2.ln_2.bias torch.Size([768])
visual.transformer.resblocks.3.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.3.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.3.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.3.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.3.ln_1.weight torch.Size([768])
visual.transformer.resblocks.3.ln_1.bias torch.Size([768])
visual.transformer.resblocks.3.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.3.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.3.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.3.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.3.ln_2.weight torch.Size([768])
visual.transformer.resblocks.3.ln_2.bias torch.Size([768])
visual.transformer.resblocks.4.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.4.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.4.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.4.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.4.ln_1.weight torch.Size([768])
visual.transformer.resblocks.4.ln_1.bias torch.Size([768])
visual.transformer.resblocks.4.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.4.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.4.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.4.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.4.ln_2.weight torch.Size([768])
visual.transformer.resblocks.4.ln_2.bias torch.Size([768])
visual.transformer.resblocks.5.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.5.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.5.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.5.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.5.ln_1.weight torch.Size([768])
visual.transformer.resblocks.5.ln_1.bias torch.Size([768])
visual.transformer.resblocks.5.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.5.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.5.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.5.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.5.ln_2.weight torch.Size([768])
visual.transformer.resblocks.5.ln_2.bias torch.Size([768])
visual.transformer.resblocks.6.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.6.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.6.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.6.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.6.ln_1.weight torch.Size([768])
visual.transformer.resblocks.6.ln_1.bias torch.Size([768])
visual.transformer.resblocks.6.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.6.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.6.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.6.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.6.ln_2.weight torch.Size([768])
visual.transformer.resblocks.6.ln_2.bias torch.Size([768])
visual.transformer.resblocks.7.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.7.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.7.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.7.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.7.ln_1.weight torch.Size([768])
visual.transformer.resblocks.7.ln_1.bias torch.Size([768])
visual.transformer.resblocks.7.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.7.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.7.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.7.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.7.ln_2.weight torch.Size([768])
visual.transformer.resblocks.7.ln_2.bias torch.Size([768])
visual.transformer.resblocks.8.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.8.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.8.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.8.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.8.ln_1.weight torch.Size([768])
visual.transformer.resblocks.8.ln_1.bias torch.Size([768])
visual.transformer.resblocks.8.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.8.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.8.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.8.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.8.ln_2.weight torch.Size([768])
visual.transformer.resblocks.8.ln_2.bias torch.Size([768])
visual.transformer.resblocks.9.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.9.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.9.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.9.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.9.ln_1.weight torch.Size([768])
visual.transformer.resblocks.9.ln_1.bias torch.Size([768])
visual.transformer.resblocks.9.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.9.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.9.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.9.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.9.ln_2.weight torch.Size([768])
visual.transformer.resblocks.9.ln_2.bias torch.Size([768])
visual.transformer.resblocks.10.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.10.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.10.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.10.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.10.ln_1.weight torch.Size([768])
visual.transformer.resblocks.10.ln_1.bias torch.Size([768])
visual.transformer.resblocks.10.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.10.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.10.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.10.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.10.ln_2.weight torch.Size([768])
visual.transformer.resblocks.10.ln_2.bias torch.Size([768])
visual.transformer.resblocks.11.attn.in_proj_weight torch.Size([2304, 768])
visual.transformer.resblocks.11.attn.in_proj_bias torch.Size([2304])
visual.transformer.resblocks.11.attn.out_proj.weight torch.Size([768, 768])
visual.transformer.resblocks.11.attn.out_proj.bias torch.Size([768])
visual.transformer.resblocks.11.ln_1.weight torch.Size([768])
visual.transformer.resblocks.11.ln_1.bias torch.Size([768])
visual.transformer.resblocks.11.mlp.c_fc.weight torch.Size([3072, 768])
visual.transformer.resblocks.11.mlp.c_fc.bias torch.Size([3072])
visual.transformer.resblocks.11.mlp.c_proj.weight torch.Size([768, 3072])
visual.transformer.resblocks.11.mlp.c_proj.bias torch.Size([768])
visual.transformer.resblocks.11.ln_2.weight torch.Size([768])
visual.transformer.resblocks.11.ln_2.bias torch.Size([768])
visual.ln_post.weight torch.Size([768])
visual.ln_post.bias torch.Size([768])
transformer.resblocks.0.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.0.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.0.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.0.attn.out_proj.bias torch.Size([512])
transformer.resblocks.0.ln_1.weight torch.Size([512])
transformer.resblocks.0.ln_1.bias torch.Size([512])
transformer.resblocks.0.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.0.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.0.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.0.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.0.ln_2.weight torch.Size([512])
transformer.resblocks.0.ln_2.bias torch.Size([512])
transformer.resblocks.1.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.1.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.1.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.1.attn.out_proj.bias torch.Size([512])
transformer.resblocks.1.ln_1.weight torch.Size([512])
transformer.resblocks.1.ln_1.bias torch.Size([512])
transformer.resblocks.1.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.1.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.1.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.1.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.1.ln_2.weight torch.Size([512])
transformer.resblocks.1.ln_2.bias torch.Size([512])
transformer.resblocks.2.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.2.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.2.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.2.attn.out_proj.bias torch.Size([512])
transformer.resblocks.2.ln_1.weight torch.Size([512])
transformer.resblocks.2.ln_1.bias torch.Size([512])
transformer.resblocks.2.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.2.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.2.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.2.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.2.ln_2.weight torch.Size([512])
transformer.resblocks.2.ln_2.bias torch.Size([512])
transformer.resblocks.3.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.3.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.3.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.3.attn.out_proj.bias torch.Size([512])
transformer.resblocks.3.ln_1.weight torch.Size([512])
transformer.resblocks.3.ln_1.bias torch.Size([512])
transformer.resblocks.3.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.3.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.3.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.3.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.3.ln_2.weight torch.Size([512])
transformer.resblocks.3.ln_2.bias torch.Size([512])
transformer.resblocks.4.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.4.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.4.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.4.attn.out_proj.bias torch.Size([512])
transformer.resblocks.4.ln_1.weight torch.Size([512])
transformer.resblocks.4.ln_1.bias torch.Size([512])
transformer.resblocks.4.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.4.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.4.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.4.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.4.ln_2.weight torch.Size([512])
transformer.resblocks.4.ln_2.bias torch.Size([512])
transformer.resblocks.5.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.5.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.5.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.5.attn.out_proj.bias torch.Size([512])
transformer.resblocks.5.ln_1.weight torch.Size([512])
transformer.resblocks.5.ln_1.bias torch.Size([512])
transformer.resblocks.5.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.5.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.5.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.5.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.5.ln_2.weight torch.Size([512])
transformer.resblocks.5.ln_2.bias torch.Size([512])
transformer.resblocks.6.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.6.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.6.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.6.attn.out_proj.bias torch.Size([512])
transformer.resblocks.6.ln_1.weight torch.Size([512])
transformer.resblocks.6.ln_1.bias torch.Size([512])
transformer.resblocks.6.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.6.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.6.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.6.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.6.ln_2.weight torch.Size([512])
transformer.resblocks.6.ln_2.bias torch.Size([512])
transformer.resblocks.7.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.7.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.7.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.7.attn.out_proj.bias torch.Size([512])
transformer.resblocks.7.ln_1.weight torch.Size([512])
transformer.resblocks.7.ln_1.bias torch.Size([512])
transformer.resblocks.7.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.7.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.7.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.7.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.7.ln_2.weight torch.Size([512])
transformer.resblocks.7.ln_2.bias torch.Size([512])
transformer.resblocks.8.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.8.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.8.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.8.attn.out_proj.bias torch.Size([512])
transformer.resblocks.8.ln_1.weight torch.Size([512])
transformer.resblocks.8.ln_1.bias torch.Size([512])
transformer.resblocks.8.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.8.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.8.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.8.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.8.ln_2.weight torch.Size([512])
transformer.resblocks.8.ln_2.bias torch.Size([512])
transformer.resblocks.9.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.9.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.9.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.9.attn.out_proj.bias torch.Size([512])
transformer.resblocks.9.ln_1.weight torch.Size([512])
transformer.resblocks.9.ln_1.bias torch.Size([512])
transformer.resblocks.9.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.9.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.9.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.9.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.9.ln_2.weight torch.Size([512])
transformer.resblocks.9.ln_2.bias torch.Size([512])
transformer.resblocks.10.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.10.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.10.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.10.attn.out_proj.bias torch.Size([512])
transformer.resblocks.10.ln_1.weight torch.Size([512])
transformer.resblocks.10.ln_1.bias torch.Size([512])
transformer.resblocks.10.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.10.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.10.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.10.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.10.ln_2.weight torch.Size([512])
transformer.resblocks.10.ln_2.bias torch.Size([512])
transformer.resblocks.11.attn.in_proj_weight torch.Size([1536, 512])
transformer.resblocks.11.attn.in_proj_bias torch.Size([1536])
transformer.resblocks.11.attn.out_proj.weight torch.Size([512, 512])
transformer.resblocks.11.attn.out_proj.bias torch.Size([512])
transformer.resblocks.11.ln_1.weight torch.Size([512])
transformer.resblocks.11.ln_1.bias torch.Size([512])
transformer.resblocks.11.mlp.c_fc.weight torch.Size([2048, 512])
transformer.resblocks.11.mlp.c_fc.bias torch.Size([2048])
transformer.resblocks.11.mlp.c_proj.weight torch.Size([512, 2048])
transformer.resblocks.11.mlp.c_proj.bias torch.Size([512])
transformer.resblocks.11.ln_2.weight torch.Size([512])
transformer.resblocks.11.ln_2.bias torch.Size([512])
token_embedding.weight torch.Size([49408, 512])
ln_final.weight torch.Size([512])
ln_final.bias torch.Size([512])
